{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural ODEs as the deep limit of ResNets\n",
    "## Part II: ResNets, Neural ODEs and limits\n",
    "### Benny Avelin (J. work with Kaj Nyström)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Overview\n",
    "\n",
    "* What is a ResNet\n",
    "* What is a Neural ODE\n",
    "* What is the limit in the title?\n",
    "* What are our results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ImageNet Large Scale Visual Recognition Challenge\n",
    "## (ILSVRC or ImageNet)\n",
    "* Over 14 Million hand annotated images, more than 20,000 categories.\n",
    "* Ran 2010-2017\n",
    "<p><a href=\"https://commons.wikimedia.org/wiki/File:ImageNet_error_rate_history_(just_systems).svg#/media/File:ImageNet_error_rate_history_(just_systems).svg\"><center><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/4f/ImageNet_error_rate_history_%28just_systems%29.svg/1200px-ImageNet_error_rate_history_%28just_systems%29.svg.png\" width=500px alt=\"ImageNet error rate history (just systems).svg\"></center></a><br>By <a href=\"//commons.wikimedia.org/w/index.php?title=User:Gkrusze&amp;action=edit&amp;redlink=1\" class=\"new\" title=\"User:Gkrusze (page does not exist)\">Gkrusze</a> - <span class=\"int-own-work\" lang=\"en\">Own work</span>, <a href=\"https://creativecommons.org/licenses/by-sa/4.0\" title=\"Creative Commons Attribution-Share Alike 4.0\">CC BY-SA 4.0</a>, <a href=\"https://commons.wikimedia.org/w/index.php?curid=69750373\">Link</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2015 winner, ResNet \n",
    "#### He, Zhang, Ren, Sun\n",
    "<center>\n",
    "    <img src=\"ResNetBlock.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ResNet\n",
    "<img src=\"ResNet.png\" widht=900px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Call $y_{n-1}$ as the input\n",
    "* Call $\\mathcal{F}$ as the `weight+relu+weight` with weights $\\theta_{n-1}$.\n",
    "* Call $\\sigma$ the ReLU\n",
    "$$\n",
    "    y_n = \\sigma (y_{n-1} + \\mathcal{F}(y_{n-1},\\theta_{n-1})\n",
    "$$\n",
    "\n",
    "# ResNet\n",
    "* this is almost a discrete ODE, lets remove the outer ReLU\n",
    "$$\n",
    "    y_n-y_{n-1} = \\mathcal{F}(y_{n-1},\\theta_{n-1})\n",
    "$$\n",
    "\n",
    "* Euler discretization of the ODE\n",
    "$$\n",
    "    \\dot{y_t} = \\mathcal{F}(y_t,\\theta_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NeuralODEs (2018)\n",
    "#### Chen, Rubanova, Bettencourt, Duvenaud\n",
    "$$\n",
    "    \\dot{y_t} = f(y_t,t,\\theta)\n",
    "$$\n",
    "* $f$ is the network\n",
    "* $y$ is the ODE solution\n",
    "* $\\theta$ are the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Problem is that if we discretize this and want to compute the gradient, the memory requirements scales with the number of time steps!\n",
    "* Their idea, use an adjoint ODE to compute the gradient of the loss, constant memory req."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep limit\n",
    "\n",
    "> Q: Does the SGD for ResNet converge to the SGD for the Neural ODE as the number of layers tend to infinity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Finite layer version\n",
    "$$\n",
    "\\begin{align}\n",
    "\t\\label{diffeq}\n",
    "\tx^{(N)}_{i+1}(x,\\theta)&= x^{(N)}_i(x,\\theta) + \\frac{1}{N} f_\\theta(x^{(N)}_i(x,\\theta)),\\ i = 0,\\ldots, N-1,\\notag\\\\\n",
    "x^{(N)}_0(x,\\theta) &= x.\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Limit problem (Neural ODE)\n",
    "$$\n",
    "\\begin{align}\n",
    "\t\\label{ODE}\n",
    "\t\\dot{x}(t)&=f_\\theta(x(t)),\\ t\\in (0,1],\\ x(0)=x,\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Corresponding risks\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\t\\mathcal{R}^{(N)}(\\theta) := \\mathbb{E}_{(x,y) \\sim \\mu} \\left [ \\|y-x^{(N)}_N(x,\\theta)\\|^2\\right ]+\\gamma H(\\theta)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\t\\mathcal{R}(\\theta) := \\mathbb{E}_{(x,y) \\sim \\mu} \\left [ \\|y-x(1,x,\\theta)\\|^2\\right ]+\\gamma H(\\theta).\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$H$ is a convex penalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The corresponding SGD approximation\n",
    "\n",
    "$$\n",
    "\\begin{align}\\label{sgdsys}\n",
    "\td\\theta^{(N)}_t &= - \\nabla \\mathcal{R}^{(N)}(\\theta^{(N)}_t) + \\Sigma dW_t,\\notag \\\\\n",
    "\td\\theta_t &= - \\nabla \\mathcal{R}(\\theta_t) + \\Sigma dW_t,\n",
    "\\end{align}\n",
    "$$\n",
    "for $t \\in [0,T]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Q: In what sense does $\\theta_t^{(N)} \\to \\theta_t$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Thm (A, Nyström):\n",
    "There exists a penalization $H$ such that\n",
    "$$\n",
    "    \\begin{align*}\n",
    "\t\t\\sup_{t\\in[0,T]} \\|\\theta_t-\\theta^{(N)}_t\\| \\to 0 \\quad \\text{in probability as $N \\to \\infty$}\n",
    "\t\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\t\t\\mathbb{E} [\\mathcal{R}(\\theta_T)] < \\infty, \\quad \\mathbb{E} [\\mathcal{R}^{(N)}(\\theta^{(N)}_T)] < \\infty.\n",
    "\t\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "> Q: Can we infer anything about $\\mathcal{R}^{(N)}(\\theta_T^{(N)}) \\to \\mathcal{R}(\\theta_T)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The problem is the interplay between the rapid growth of $\\mathcal{R}$ and we seem to loose control of the difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* One could argue that this problem is purely academic, in reality we are working with numbers that have a maximum representable value.\n",
    "\n",
    "### Additional assumption (Capped model)\n",
    "$$\n",
    "\\begin{align}\\label{eq:modified_risk-}\n",
    "\\tilde {\\mathcal{R}}^{(N)}(\\theta)&:= \\mathbb{E}_{(x,y) \\sim \\mu} \\left [ \\|y-x^{(N)}(1,T_{\\Lambda}(\\theta))\\|^2\\right ]+\\gamma H(\\theta),\\notag\\\\\n",
    "\\tilde {\\mathcal{R}}(\\theta)&:= \\mathbb{E}_{(x,y) \\sim \\mu} \\left [ \\|y-x(1,T_{\\Lambda}(\\theta))\\|^2\\right ]+\\gamma H(\\theta),\n",
    "\\end{align}\n",
    "$$\n",
    "$H$ is a convex potential with quadradic growth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Thm (A, Nyström): (Capped Model)\n",
    "Assume that the initial density $p_0$ of $\\theta_t,\\theta^{(N)}_t$ is compactly supported, then\n",
    "$$\n",
    "\\begin{align}\n",
    "\t\t\\label{thm2.4a}\n",
    "\t\t\\sup_{t\\in[0,T]}\\bigl \\|\\mathbb{E}[\\theta_t-\\theta^{(N)}_t]\\bigr \\|&\\leq   c N^{-1}\\|p_0\\|_2,\\\\\n",
    "\t\t\\label{thm2.4b}\n",
    "\t\t\\sup_{t\\in[0,T]}\\bigl |\\mathbb{E}[\\tilde {\\mathcal{R}}(\\theta_t)-\\tilde {\\mathcal{R}}^{(N)}(\\theta^{(N)}_t) ]\\bigr |&\\leq  c N^{-1}\\|p_0\\|_2.\n",
    "\t\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let $p(\\theta,t),p^{(N)}(\\theta)$ be the probability densities of $\\theta_t, \\theta_t^{(N)}$\n",
    "$$\n",
    "\\begin{align} \\label{esta1}\n",
    "\t\\int\\limits_{\\mathbb R^m}e^{\\gamma H(\\theta)/4}|p(\\theta,T)-p^{(N)}(\\theta,T)|\\, d\\theta&\\leq  c N^{-1}\\|p_0\\|_2,\\\\\n",
    "\\int\\limits_{B(0, 2^{k+1}\\tilde R_0)\\setminus B(0, 2^{k}\\tilde R_0)}e^{\\gamma H(\\theta)/4}|p(\\theta,T)-p^{(N)}(\\theta,T)|\\, d\\theta&\\leq  c e^{-2^k\\tilde R_0^2/T} N^{-1}\\|p_0\\|_2, \\notag\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Cifar10\n",
    "60000 32x32 colour images in 10 classes, with 6000 images per class\n",
    "\n",
    "<center>\n",
    "<img src=\"cifar10_plot.png\"></img>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Numerical experiment\n",
    "![CIFAR10_ACC.PNG](CIFAR10_ACC.PNG)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
